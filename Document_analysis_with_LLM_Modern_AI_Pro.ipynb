{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kishdas/from_modernaipro/blob/main/Document_analysis_with_LLM_Modern_AI_Pro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\#Modern AI Pro: Document analysis with LLMs\n",
        "### Step 1: Setup the basics for processing document:\n",
        "Get one of our [research papers](https://drive.google.com/file/d/1kfjF9iuGG74ORFGu4v9dMO15pgsKI5Rh/view?usp=sharing) for sample. Download a copy locally and upload to the runtime."
      ],
      "metadata": {
        "id": "H7K1iJrcKAPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will use a simple utility to make the text wrap properly when printing.\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "K7cuee6jMg02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "c72f9caf-7329-4f70-ef9c-0a8d2b0766bb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read pages of the document\n",
        "!pip install -q -U pypdf2\n",
        "from PyPDF2 import PdfReader\n",
        "reader = PdfReader('arso1.pdf')\n",
        "text = \"\"\n",
        "for i in range(0, len(reader.pages)):\n",
        "    page = reader.pages[i]\n",
        "    text += page.extract_text() + \" \""
      ],
      "metadata": {
        "id": "d_WgdFtm4Vgx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "a3638d91-c03a-48e5-dbe8-3fe532077a0e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m174.1/232.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'arso1.pdf'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-150ef5198f34>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install -q -U pypdf2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPyPDF2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPdfReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPdfReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'arso1.pdf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PyPDF2/_reader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, stream, strict, password)\u001b[0m\n\u001b[1;32m    315\u001b[0m             )\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m                 \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'arso1.pdf'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def display_word_cloud(top_100_words):\n",
        "  wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(dict(top_100_words))\n",
        "\n",
        "  plt.figure(figsize=(10, 5))\n",
        "  plt.imshow(wordcloud, interpolation='bilinear')\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "ogQQG1FrlbJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Visualize the data"
      ],
      "metadata": {
        "id": "qX_66QfOJFVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "text = text.lower()\n",
        "words = text.split()\n",
        "word_counts = Counter(words)\n",
        "top_100_words = word_counts.most_common(100)\n",
        "\n",
        "display_word_cloud(top_100_words)"
      ],
      "metadata": {
        "id": "lWiN57xJheuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That is a lot of just common words. Let's remove them and display again."
      ],
      "metadata": {
        "id": "WE8vcdJOkkDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Filter out stop words from your list of words\n",
        "filtered_words = [word for word in words if word not in stop_words]\n",
        "word_counts_filtered = Counter(filtered_words)\n",
        "\n",
        "# If you still want to limit it to the top 100 words\n",
        "top_100_words_filtered = word_counts_filtered.most_common(100)\n",
        "\n",
        "display_word_cloud(top_100_words_filtered)"
      ],
      "metadata": {
        "id": "sg7QHhsZkgOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatize to group similar words"
      ],
      "metadata": {
        "id": "7weJ55cSl0Gw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "tokens = word_tokenize(text)  # Tokenize the text\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Lemmatize tokens and remove stop words\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words and token.isalpha()]\n",
        "\n",
        "# Recount words\n",
        "word_counts = Counter(lemmatized_tokens)\n",
        "\n",
        "# Extract the top 100 words\n",
        "top_100_words_lemmatized = word_counts.most_common(100)\n",
        "display_word_cloud(top_100_words_lemmatized)"
      ],
      "metadata": {
        "id": "Y2KSmn-DlpND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Storing the docs in Vector DB"
      ],
      "metadata": {
        "id": "7DKSjrqSKOql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split the texts into small chunks**"
      ],
      "metadata": {
        "id": "GB1Xt0mPLJH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U langchain langchainhub langchain-community chromadb sentence-transformers\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema.document import Document\n",
        "\n",
        "documents = [Document(page_content=text, metadata={\"source\": \"local\"})]\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=40)\n",
        "all_splits = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "MhGcX4AnKFVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\", model_kwargs={\"device\": \"cpu\"})"
      ],
      "metadata": {
        "id": "QTd4XDW8Kpfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "vectordb_paper = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory=\"chroma_db_paper\")"
      ],
      "metadata": {
        "id": "UV4d7vjhK4gH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Tell me about likability index\"\n",
        "docs = vectordb_paper.similarity_search(query)\n",
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "id": "bjpyl21vPDkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever_paper = vectordb_paper.as_retriever()"
      ],
      "metadata": {
        "id": "_ExiVlWbOxks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Setting up the LLM"
      ],
      "metadata": {
        "id": "k0ECSpGxm7oO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q langchain-groq\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from langchain_groq import ChatGroq\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get(\"GROQ_API_KEY\")\n",
        "llm_groq = ChatGroq(model_name=\"llama3-70b-8192\")"
      ],
      "metadata": {
        "id": "c_vr-Oarpa5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "qa_paper = RetrievalQA.from_chain_type(\n",
        "    llm_groq,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever_paper,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "jiN-V67M6JuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_manager(qa, query):\n",
        "    print(\"\\nResult: \", qa.run(query))"
      ],
      "metadata": {
        "id": "JfU7GCDy67s3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_manager(qa_paper,\"Tell me about the likability index\")"
      ],
      "metadata": {
        "id": "IeI0HTr16fCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: You can analyze any piece of text now."
      ],
      "metadata": {
        "id": "sRQzyi2M6w4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news = \"\"\"SRINAGAR, India (AP) — For decades, India has focused its defense policy on its land borders with rivals Pakistan and China. Now, as its global ambitions expand, it is beginning to flex its naval power in international waters, including anti-piracy patrols and a widely publicized deployment close to the Red Sea to help protect ships from attacks during Israel’s war with Hamas.\n",
        "\n",
        "India sent three guided missile destroyers and reconnaissance aircraft in November when Yemen-based Houthi rebels began targeting ships in solidarity with Hamas, causing disruptions in a key trading route that handles about 12% of global trade.\n",
        "\n",
        "The deployment highlights the country as a “proactive contributor” to international maritime stability, said Vice Adm. Anil Kumar Chawla, who retired in 2021 as head of India’s southern naval command.\n",
        "\n",
        "\n",
        "“We are not doing it only out of altruism. Unless you are a maritime power you can never aspire to be a global power,” Chawla said. India, already a regional power, is positioning itself “as a global player today, an upcoming global power,” he said.\n",
        "India is widely publicizing the deployments, signaling its desire to assume a wider responsibility in maritime security to the world and its growing maritime ambitions to regional rival China.\n",
        "\n",
        "“It is a message to China that, look, we can deploy such a large force here. This is our backyard. Though we don’t own it, but we are probably the most capable and responsible resident naval power,” Chawla said.\n",
        "\n",
        "The Indian navy has helped at least four ships, three of which were attacked by Houthi rebels and another that Washington blamed on Iran, a charge denied by Tehran. It has also conducted several anti-piracy missions.\"\"\"\n",
        "\n",
        "documents = [Document(page_content=news, metadata={\"source\": \"local\"})]\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
        "all_splits = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "Jpf-26fLsDAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory=\"chroma_db\")\n",
        "retriever = vectordb.as_retriever()\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm= llm_groq,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "STaj48NqxfOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_manager(qa, \"\"\" What are all the key countries involved in this? comment on the geopolitics behind it. \"\"\")"
      ],
      "metadata": {
        "id": "cuk_jRW4xoN0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}